{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes from run-1\n",
    "\n",
    "#### Test data\n",
    "* Previously we used cross-validation to select the best model, and tested that model on data (opus 131) that we had kept apart from the beginning.\n",
    "* The scores achieved when predicting on opus 131 were our final results.\n",
    "* This time we train/validate on all data and take the final cross-validation scores as our results.\n",
    "\n",
    "#### Crossvalidation\n",
    "* Previously we simply took all sequences in the train/validate set, shuffled them and trained/validated with a 80/20 split.\n",
    "* This time we instead shuffle the opuses (opi?) before generating the sequences, with the idea that this could hint at how patterns generalize across opuses with. So we basically have leave one (opus) out cross validation.\n",
    "\n",
    "#### Input\n",
    "* Previously we grouped similar chords together and grouped chords that appeared rarely (less than 10 times) under a single label.\n",
    "* The idea was to remove outliers, reduce the output space and improve generalization\n",
    "* As it was indicated that having the amount of output classes be dependent on the input was a bad idea we now use rules independent of the data for grouping and have ~800 classes instead of ~100\n",
    "\n",
    "#### Model\n",
    "* Previously we had a bi-directional LSTM layer in the model architecture as it increased performance. For the sake of being able to compare the results to a simple N-gram model we decided to remove that layer in this iteration.\n",
    "\n",
    "#### Hyperparameters\n",
    "* Given the increased amount of outliers and the removal of the bidirectional layer we expect generalization accuracy to decrease.\n",
    "* To remedy that we used the current model and iterated through different values for a regularization parameter, which we didn't explore previously.\n",
    "* Best scores were obtained with regstrength = 0 however.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigis/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from chord_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "#Load all data\n",
    "data = pd.read_csv('data/820chords.csv')\n",
    "\n",
    "#Remove redundant attributes. Keep op to split into opuses\n",
    "data = data[['chord', 'op']]\n",
    "\n",
    "#Use dummy variable representation for the chords\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(lstm_x, lstm_y, optimizer, loss, metrics, regstrength):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences=True, input_shape=(lstm_x.shape[1], lstm_x.shape[2]),\\\n",
    "                   kernel_regularizer=regularizers.l2(regstrength)))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False,\\\n",
    "              kernel_regularizer=regularizers.l2(regstrength)))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(lstm_y.shape[1], activation='softmax', \\\n",
    "             kernel_regularizer=regularizers.l2(regstrength)))\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters for the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'Adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "epochs = 30\n",
    "verbose = 2\n",
    "seq_length = 10\n",
    "\n",
    "#Save the weights whenever validation accuracy is increased\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'weights.{epoch:02d}-{val_acc:.4f}.hdf5',\n",
    "    monitor='val_acc', \n",
    "    verbose=0,        \n",
    "    save_best_only=False\n",
    ")\n",
    "# Stop the learning process if we havent improved validation accuracy for 10 epochs\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1)\n",
    "\n",
    "#callbacks_list = [checkpoint, earlystop]   \n",
    "callbacks_list = [earlystop]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "\n",
      "Checking regstrength 0\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 156s - loss: 4.0774 - acc: 0.1196 - val_loss: 4.1609 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 156s - loss: 3.9651 - acc: 0.1245 - val_loss: 4.1684 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 153s - loss: 3.9606 - acc: 0.1257 - val_loss: 4.1837 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 156s - loss: 3.9521 - acc: 0.1265 - val_loss: 4.1703 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 149s - loss: 3.9519 - acc: 0.1277 - val_loss: 4.1854 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 149s - loss: 3.9484 - acc: 0.1270 - val_loss: 4.1873 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 150s - loss: 4.1173 - acc: 0.1189 - val_loss: 3.7201 - val_acc: 0.1530\n",
      "Epoch 2/30\n",
      " - 148s - loss: 4.0046 - acc: 0.1227 - val_loss: 3.7432 - val_acc: 0.1530\n",
      "Epoch 3/30\n",
      " - 148s - loss: 3.9974 - acc: 0.1253 - val_loss: 3.7355 - val_acc: 0.1530\n",
      "Epoch 4/30\n",
      " - 148s - loss: 3.9954 - acc: 0.1244 - val_loss: 3.7466 - val_acc: 0.1530\n",
      "Epoch 5/30\n",
      " - 148s - loss: 3.9939 - acc: 0.1251 - val_loss: 3.7472 - val_acc: 0.1530\n",
      "Epoch 6/30\n",
      " - 148s - loss: 4.0158 - acc: 0.1248 - val_loss: 3.7515 - val_acc: 0.1530\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 150s - loss: 4.0920 - acc: 0.1198 - val_loss: 3.9881 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 148s - loss: 3.9874 - acc: 0.1244 - val_loss: 4.0271 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 148s - loss: 3.9800 - acc: 0.1263 - val_loss: 4.0413 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 148s - loss: 3.9721 - acc: 0.1265 - val_loss: 4.0406 - val_acc: 0.1312\n",
      "Epoch 5/30\n",
      " - 148s - loss: 3.9880 - acc: 0.1277 - val_loss: 4.0504 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 148s - loss: 3.9483 - acc: 0.1310 - val_loss: 4.0344 - val_acc: 0.1359\n",
      "Epoch 7/30\n",
      " - 148s - loss: 3.9266 - acc: 0.1343 - val_loss: 4.0037 - val_acc: 0.1421\n",
      "Epoch 8/30\n",
      " - 148s - loss: 3.9081 - acc: 0.1370 - val_loss: 3.9970 - val_acc: 0.1425\n",
      "Epoch 9/30\n",
      " - 148s - loss: 3.8968 - acc: 0.1385 - val_loss: 4.0121 - val_acc: 0.1390\n",
      "Epoch 10/30\n",
      " - 148s - loss: 3.8797 - acc: 0.1421 - val_loss: 3.9818 - val_acc: 0.1492\n",
      "Epoch 11/30\n",
      " - 148s - loss: 3.8610 - acc: 0.1453 - val_loss: 3.9724 - val_acc: 0.1472\n",
      "Epoch 12/30\n",
      " - 152s - loss: 3.8142 - acc: 0.1528 - val_loss: 3.9271 - val_acc: 0.1527\n",
      "Epoch 13/30\n",
      " - 160s - loss: 3.7736 - acc: 0.1577 - val_loss: 3.8937 - val_acc: 0.1605\n",
      "Epoch 14/30\n",
      " - 153s - loss: 3.7495 - acc: 0.1612 - val_loss: 3.8818 - val_acc: 0.1543\n",
      "Epoch 15/30\n",
      " - 152s - loss: 3.7158 - acc: 0.1669 - val_loss: 3.8805 - val_acc: 0.1590\n",
      "Epoch 16/30\n",
      " - 154s - loss: 3.6641 - acc: 0.1757 - val_loss: 3.8411 - val_acc: 0.1691\n",
      "Epoch 17/30\n",
      " - 148s - loss: 3.6142 - acc: 0.1831 - val_loss: 3.8233 - val_acc: 0.1660\n",
      "Epoch 18/30\n",
      " - 148s - loss: 3.5672 - acc: 0.1933 - val_loss: 3.8429 - val_acc: 0.1754\n",
      "Epoch 19/30\n",
      " - 148s - loss: 3.5262 - acc: 0.1983 - val_loss: 3.8310 - val_acc: 0.1809\n",
      "Epoch 20/30\n",
      " - 148s - loss: 3.4856 - acc: 0.2020 - val_loss: 3.8231 - val_acc: 0.1887\n",
      "Epoch 21/30\n",
      " - 148s - loss: 3.4446 - acc: 0.2106 - val_loss: 3.8526 - val_acc: 0.1922\n",
      "Epoch 22/30\n",
      " - 148s - loss: 3.4045 - acc: 0.2179 - val_loss: 3.8518 - val_acc: 0.1836\n",
      "Epoch 23/30\n",
      " - 148s - loss: 3.3646 - acc: 0.2235 - val_loss: 3.8728 - val_acc: 0.1801\n",
      "Epoch 24/30\n",
      " - 148s - loss: 3.3281 - acc: 0.2298 - val_loss: 3.8507 - val_acc: 0.1864\n",
      "Epoch 25/30\n",
      " - 148s - loss: 3.2787 - acc: 0.2367 - val_loss: 3.9013 - val_acc: 0.1875\n",
      "Epoch 26/30\n",
      " - 148s - loss: 3.2448 - acc: 0.2418 - val_loss: 3.9721 - val_acc: 0.1695\n",
      "Epoch 00026: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 151s - loss: 4.0421 - acc: 0.1201 - val_loss: 4.5479 - val_acc: 0.1139\n",
      "Epoch 2/30\n",
      " - 148s - loss: 3.9305 - acc: 0.1242 - val_loss: 4.6234 - val_acc: 0.1139\n",
      "Epoch 3/30\n",
      " - 148s - loss: 3.9232 - acc: 0.1290 - val_loss: 4.6610 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 148s - loss: 3.9219 - acc: 0.1289 - val_loss: 4.6784 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 149s - loss: 3.9211 - acc: 0.1273 - val_loss: 4.7059 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 149s - loss: 3.9153 - acc: 0.1275 - val_loss: 4.7074 - val_acc: 0.1139\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 156s - loss: 4.0741 - acc: 0.1183 - val_loss: 4.2304 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 154s - loss: 3.9677 - acc: 0.1227 - val_loss: 4.2794 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 154s - loss: 3.9592 - acc: 0.1256 - val_loss: 4.2735 - val_acc: 0.1564\n",
      "Epoch 4/30\n",
      " - 154s - loss: 3.9522 - acc: 0.1258 - val_loss: 4.2655 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 167s - loss: 3.9303 - acc: 0.1304 - val_loss: 4.2328 - val_acc: 0.1570\n",
      "Epoch 6/30\n",
      " - 159s - loss: 3.8512 - acc: 0.1462 - val_loss: 4.1638 - val_acc: 0.1610\n",
      "Epoch 7/30\n",
      " - 168s - loss: 3.7621 - acc: 0.1585 - val_loss: 4.1379 - val_acc: 0.1610\n",
      "Epoch 8/30\n",
      " - 163s - loss: 3.7067 - acc: 0.1619 - val_loss: 4.1199 - val_acc: 0.1590\n",
      "Epoch 9/30\n",
      " - 177s - loss: 3.6358 - acc: 0.1748 - val_loss: 4.1098 - val_acc: 0.1783\n",
      "Epoch 10/30\n",
      " - 173s - loss: 3.5775 - acc: 0.1901 - val_loss: 4.1158 - val_acc: 0.1703\n",
      "Epoch 11/30\n",
      " - 191s - loss: 3.5201 - acc: 0.1985 - val_loss: 4.1139 - val_acc: 0.1657\n",
      "Epoch 12/30\n",
      " - 175s - loss: 3.4678 - acc: 0.2097 - val_loss: 4.1330 - val_acc: 0.1717\n",
      "Epoch 13/30\n",
      " - 157s - loss: 3.4144 - acc: 0.2180 - val_loss: 4.1410 - val_acc: 0.1776\n",
      "Epoch 14/30\n",
      " - 157s - loss: 3.3590 - acc: 0.2274 - val_loss: 4.1508 - val_acc: 0.1823\n",
      "Epoch 15/30\n",
      " - 157s - loss: 3.3125 - acc: 0.2324 - val_loss: 4.1560 - val_acc: 0.1730\n",
      "Epoch 16/30\n",
      " - 157s - loss: 3.2575 - acc: 0.2426 - val_loss: 4.1679 - val_acc: 0.1796\n",
      "Epoch 17/30\n",
      " - 157s - loss: 3.2048 - acc: 0.2482 - val_loss: 4.2341 - val_acc: 0.1710\n",
      "Epoch 18/30\n",
      " - 157s - loss: 3.1437 - acc: 0.2567 - val_loss: 4.2689 - val_acc: 0.1796\n",
      "Epoch 19/30\n",
      " - 157s - loss: 3.0917 - acc: 0.2684 - val_loss: 4.3180 - val_acc: 0.1637\n",
      "Epoch 00019: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 130s - loss: 4.1764 - acc: 0.1164 - val_loss: 3.8380 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 126s - loss: 4.0356 - acc: 0.1161 - val_loss: 3.8556 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 126s - loss: 4.0306 - acc: 0.1194 - val_loss: 3.8728 - val_acc: 0.1432\n",
      "Epoch 4/30\n",
      " - 125s - loss: 4.0271 - acc: 0.1217 - val_loss: 3.8724 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 125s - loss: 4.0229 - acc: 0.1206 - val_loss: 3.8713 - val_acc: 0.1432\n",
      "Epoch 6/30\n",
      " - 125s - loss: 4.0259 - acc: 0.1213 - val_loss: 3.8805 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 143s - loss: 4.1095 - acc: 0.1256 - val_loss: 3.9876 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 139s - loss: 3.9756 - acc: 0.1326 - val_loss: 4.0011 - val_acc: 0.1049\n",
      "Epoch 3/30\n",
      " - 139s - loss: 3.9686 - acc: 0.1328 - val_loss: 3.9922 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 139s - loss: 3.9410 - acc: 0.1342 - val_loss: 4.0061 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 139s - loss: 3.8783 - acc: 0.1443 - val_loss: 3.8876 - val_acc: 0.1414\n",
      "Epoch 6/30\n",
      " - 139s - loss: 3.8069 - acc: 0.1557 - val_loss: 3.8174 - val_acc: 0.1587\n",
      "Epoch 7/30\n",
      " - 139s - loss: 3.7516 - acc: 0.1591 - val_loss: 3.8092 - val_acc: 0.1617\n",
      "Epoch 8/30\n",
      " - 139s - loss: 3.6981 - acc: 0.1708 - val_loss: 3.7980 - val_acc: 0.1707\n",
      "Epoch 9/30\n",
      " - 139s - loss: 3.6451 - acc: 0.1830 - val_loss: 3.7747 - val_acc: 0.1837\n",
      "Epoch 10/30\n",
      " - 139s - loss: 3.5876 - acc: 0.1885 - val_loss: 3.7412 - val_acc: 0.1788\n",
      "Epoch 11/30\n",
      " - 139s - loss: 3.5284 - acc: 0.1995 - val_loss: 3.7466 - val_acc: 0.1876\n",
      "Epoch 12/30\n",
      " - 139s - loss: 3.4740 - acc: 0.2096 - val_loss: 3.7339 - val_acc: 0.1964\n",
      "Epoch 13/30\n",
      " - 139s - loss: 3.4194 - acc: 0.2188 - val_loss: 3.7410 - val_acc: 0.1984\n",
      "Epoch 14/30\n",
      " - 139s - loss: 3.3652 - acc: 0.2255 - val_loss: 3.7593 - val_acc: 0.2015\n",
      "Epoch 15/30\n",
      " - 139s - loss: 3.3092 - acc: 0.2337 - val_loss: 3.7739 - val_acc: 0.2045\n",
      "Epoch 16/30\n",
      " - 140s - loss: 3.2561 - acc: 0.2418 - val_loss: 3.7690 - val_acc: 0.2054\n",
      "Epoch 17/30\n",
      " - 139s - loss: 3.1979 - acc: 0.2530 - val_loss: 3.8071 - val_acc: 0.1946\n",
      "Epoch 18/30\n",
      " - 139s - loss: 3.1386 - acc: 0.2616 - val_loss: 3.8239 - val_acc: 0.1973\n",
      "Epoch 19/30\n",
      " - 139s - loss: 3.0756 - acc: 0.2665 - val_loss: 3.8388 - val_acc: 0.2040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      " - 138s - loss: 3.0211 - acc: 0.2802 - val_loss: 3.8871 - val_acc: 0.2081\n",
      "Epoch 21/30\n",
      " - 139s - loss: 2.9544 - acc: 0.2908 - val_loss: 3.9298 - val_acc: 0.1984\n",
      "Epoch 22/30\n",
      " - 139s - loss: 2.8910 - acc: 0.3009 - val_loss: 3.9916 - val_acc: 0.1923\n",
      "Epoch 23/30\n",
      " - 138s - loss: 2.8284 - acc: 0.3142 - val_loss: 4.0329 - val_acc: 0.1869\n",
      "Epoch 24/30\n",
      " - 138s - loss: 2.7660 - acc: 0.3216 - val_loss: 4.0875 - val_acc: 0.1869\n",
      "Epoch 25/30\n",
      " - 138s - loss: 2.7001 - acc: 0.3317 - val_loss: 4.1294 - val_acc: 0.1838\n",
      "Epoch 00025: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 160s - loss: 4.0954 - acc: 0.1210 - val_loss: 3.8079 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 156s - loss: 3.9852 - acc: 0.1225 - val_loss: 3.8016 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 156s - loss: 3.9804 - acc: 0.1257 - val_loss: 3.8097 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 156s - loss: 3.9787 - acc: 0.1279 - val_loss: 3.8232 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 155s - loss: 3.9772 - acc: 0.1271 - val_loss: 3.8217 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 155s - loss: 3.9778 - acc: 0.1264 - val_loss: 3.7964 - val_acc: 0.1406\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 161s - loss: 4.0461 - acc: 0.1243 - val_loss: 4.7996 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 157s - loss: 3.9448 - acc: 0.1289 - val_loss: 4.8370 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 157s - loss: 3.9405 - acc: 0.1303 - val_loss: 4.8754 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 157s - loss: 3.9524 - acc: 0.1295 - val_loss: 4.9409 - val_acc: 0.0806\n",
      "Epoch 5/30\n",
      " - 157s - loss: 3.9570 - acc: 0.1295 - val_loss: 4.8648 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 157s - loss: 3.9092 - acc: 0.1311 - val_loss: 4.8596 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Checking regstrength 0.001\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 160s - loss: 4.2232 - acc: 0.1189 - val_loss: 4.2119 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 155s - loss: 4.0214 - acc: 0.1237 - val_loss: 4.1671 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 155s - loss: 4.0083 - acc: 0.1267 - val_loss: 4.1770 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 155s - loss: 3.9845 - acc: 0.1259 - val_loss: 4.1644 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 191s - loss: 3.9751 - acc: 0.1268 - val_loss: 4.1759 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 169s - loss: 3.9722 - acc: 0.1267 - val_loss: 4.1611 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 170s - loss: 4.2686 - acc: 0.1183 - val_loss: 3.7804 - val_acc: 0.1530\n",
      "Epoch 2/30\n",
      " - 155s - loss: 4.0653 - acc: 0.1216 - val_loss: 3.7314 - val_acc: 0.1530\n",
      "Epoch 3/30\n",
      " - 155s - loss: 4.0331 - acc: 0.1244 - val_loss: 3.7230 - val_acc: 0.1530\n",
      "Epoch 4/30\n",
      " - 155s - loss: 4.0218 - acc: 0.1241 - val_loss: 3.7321 - val_acc: 0.1530\n",
      "Epoch 5/30\n",
      " - 155s - loss: 4.0131 - acc: 0.1249 - val_loss: 3.7171 - val_acc: 0.1530\n",
      "Epoch 6/30\n",
      " - 155s - loss: 4.0133 - acc: 0.1234 - val_loss: 3.7202 - val_acc: 0.1530\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 161s - loss: 4.2546 - acc: 0.1194 - val_loss: 3.9999 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 155s - loss: 4.0451 - acc: 0.1245 - val_loss: 3.9848 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 155s - loss: 4.0113 - acc: 0.1259 - val_loss: 3.9758 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 155s - loss: 3.9990 - acc: 0.1260 - val_loss: 3.9521 - val_acc: 0.1316\n",
      "Epoch 5/30\n",
      " - 156s - loss: 3.9925 - acc: 0.1272 - val_loss: 3.9485 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 155s - loss: 4.4027 - acc: 0.1265 - val_loss: 4.0197 - val_acc: 0.1316\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 163s - loss: 4.1972 - acc: 0.1211 - val_loss: 4.4901 - val_acc: 0.1139\n",
      "Epoch 2/30\n",
      " - 158s - loss: 3.9872 - acc: 0.1262 - val_loss: 4.4945 - val_acc: 0.0569\n",
      "Epoch 3/30\n",
      " - 157s - loss: 3.9784 - acc: 0.1273 - val_loss: 4.4857 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 157s - loss: 3.9437 - acc: 0.1279 - val_loss: 4.4907 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 157s - loss: 3.9373 - acc: 0.1282 - val_loss: 4.5115 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 157s - loss: 3.9391 - acc: 0.1289 - val_loss: 4.5073 - val_acc: 0.1139\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 169s - loss: 4.2230 - acc: 0.1170 - val_loss: 4.2178 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 162s - loss: 4.0234 - acc: 0.1210 - val_loss: 4.1637 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 162s - loss: 3.9963 - acc: 0.1229 - val_loss: 4.1986 - val_acc: 0.1564\n",
      "Epoch 4/30\n",
      " - 162s - loss: 3.9838 - acc: 0.1229 - val_loss: 4.2110 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 162s - loss: 3.9804 - acc: 0.1241 - val_loss: 4.1918 - val_acc: 0.1564\n",
      "Epoch 6/30\n",
      " - 162s - loss: 3.9761 - acc: 0.1236 - val_loss: 4.2091 - val_acc: 0.1564\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 139s - loss: 4.3448 - acc: 0.1139 - val_loss: 3.9207 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 131s - loss: 4.1266 - acc: 0.1173 - val_loss: 3.8895 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 131s - loss: 4.0869 - acc: 0.1202 - val_loss: 3.8845 - val_acc: 0.1432\n",
      "Epoch 4/30\n",
      " - 131s - loss: 4.0670 - acc: 0.1217 - val_loss: 3.8669 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 131s - loss: 4.0593 - acc: 0.1215 - val_loss: 3.8459 - val_acc: 0.1432\n",
      "Epoch 6/30\n",
      " - 131s - loss: 4.0519 - acc: 0.1214 - val_loss: 3.8568 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 190s - loss: 4.2717 - acc: 0.1249 - val_loss: 4.0749 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 157s - loss: 4.0457 - acc: 0.1306 - val_loss: 4.0196 - val_acc: 0.1049\n",
      "Epoch 3/30\n",
      " - 155s - loss: 4.0239 - acc: 0.1334 - val_loss: 4.0104 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 145s - loss: 4.0094 - acc: 0.1340 - val_loss: 4.0144 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 145s - loss: 3.9954 - acc: 0.1333 - val_loss: 4.0122 - val_acc: 0.1049\n",
      "Epoch 6/30\n",
      " - 145s - loss: 3.9873 - acc: 0.1332 - val_loss: 4.0087 - val_acc: 0.1049\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 170s - loss: 4.2444 - acc: 0.1211 - val_loss: 3.8714 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 161s - loss: 4.0456 - acc: 0.1244 - val_loss: 3.8550 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 161s - loss: 4.0196 - acc: 0.1249 - val_loss: 3.8453 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 161s - loss: 4.0031 - acc: 0.1260 - val_loss: 3.8154 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 161s - loss: 3.9981 - acc: 0.1256 - val_loss: 3.8158 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 161s - loss: 3.9969 - acc: 0.1257 - val_loss: 3.8088 - val_acc: 0.1406\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 170s - loss: 4.2053 - acc: 0.1229 - val_loss: 4.6665 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 161s - loss: 4.0084 - acc: 0.1265 - val_loss: 4.6690 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 161s - loss: 3.9782 - acc: 0.1280 - val_loss: 4.6431 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 161s - loss: 3.9611 - acc: 0.1300 - val_loss: 4.6638 - val_acc: 0.0806\n",
      "Epoch 5/30\n",
      " - 161s - loss: 3.9541 - acc: 0.1285 - val_loss: 4.7268 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 161s - loss: 3.9522 - acc: 0.1292 - val_loss: 4.7021 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Checking regstrength 0.005\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 166s - loss: 4.4022 - acc: 0.1209 - val_loss: 4.2167 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 156s - loss: 4.0397 - acc: 0.1248 - val_loss: 4.2184 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 156s - loss: 4.0307 - acc: 0.1270 - val_loss: 4.2260 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 156s - loss: 4.0245 - acc: 0.1266 - val_loss: 4.2118 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 157s - loss: 4.0224 - acc: 0.1273 - val_loss: 4.2222 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 156s - loss: 4.0203 - acc: 0.1259 - val_loss: 4.2110 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 166s - loss: 4.4464 - acc: 0.1179 - val_loss: 3.7797 - val_acc: 0.1530\n",
      "Epoch 2/30\n",
      " - 156s - loss: 4.0790 - acc: 0.1230 - val_loss: 3.7896 - val_acc: 0.1530\n",
      "Epoch 3/30\n",
      " - 156s - loss: 4.0707 - acc: 0.1224 - val_loss: 3.7686 - val_acc: 0.1530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      " - 156s - loss: 4.0636 - acc: 0.1253 - val_loss: 3.7646 - val_acc: 0.0954\n",
      "Epoch 5/30\n",
      " - 156s - loss: 4.0624 - acc: 0.1232 - val_loss: 3.7766 - val_acc: 0.1530\n",
      "Epoch 6/30\n",
      " - 156s - loss: 4.0603 - acc: 0.1242 - val_loss: 3.7966 - val_acc: 0.0954\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 165s - loss: 4.4184 - acc: 0.1192 - val_loss: 4.0115 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 155s - loss: 4.0570 - acc: 0.1241 - val_loss: 3.9921 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 155s - loss: 4.0466 - acc: 0.1261 - val_loss: 4.0185 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 155s - loss: 4.0450 - acc: 0.1265 - val_loss: 4.0183 - val_acc: 0.1316\n",
      "Epoch 5/30\n",
      " - 155s - loss: 4.0389 - acc: 0.1262 - val_loss: 4.0177 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 155s - loss: 4.0387 - acc: 0.1270 - val_loss: 4.0131 - val_acc: 0.1316\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 167s - loss: 4.3735 - acc: 0.1216 - val_loss: 4.5159 - val_acc: 0.1139\n",
      "Epoch 2/30\n",
      " - 156s - loss: 4.0080 - acc: 0.1269 - val_loss: 4.5152 - val_acc: 0.1139\n",
      "Epoch 3/30\n",
      " - 156s - loss: 3.9981 - acc: 0.1274 - val_loss: 4.5176 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 156s - loss: 3.9938 - acc: 0.1249 - val_loss: 4.5288 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 156s - loss: 3.9886 - acc: 0.1283 - val_loss: 4.5560 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 156s - loss: 3.9831 - acc: 0.1275 - val_loss: 4.5530 - val_acc: 0.1139\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 171s - loss: 4.4023 - acc: 0.1203 - val_loss: 4.1988 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 160s - loss: 4.0425 - acc: 0.1237 - val_loss: 4.2023 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 160s - loss: 4.0346 - acc: 0.1237 - val_loss: 4.2288 - val_acc: 0.1564\n",
      "Epoch 4/30\n",
      " - 160s - loss: 4.0298 - acc: 0.1240 - val_loss: 4.2393 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 160s - loss: 4.0240 - acc: 0.1241 - val_loss: 4.2582 - val_acc: 0.1564\n",
      "Epoch 6/30\n",
      " - 160s - loss: 4.0219 - acc: 0.1251 - val_loss: 4.2328 - val_acc: 0.1564\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 141s - loss: 4.5807 - acc: 0.1142 - val_loss: 3.9404 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 129s - loss: 4.1237 - acc: 0.1181 - val_loss: 3.9158 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 129s - loss: 4.1070 - acc: 0.1176 - val_loss: 3.9024 - val_acc: 0.1432\n",
      "Epoch 4/30\n",
      " - 129s - loss: 4.1026 - acc: 0.1192 - val_loss: 3.9195 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 129s - loss: 4.0984 - acc: 0.1206 - val_loss: 3.9096 - val_acc: 0.1432\n",
      "Epoch 6/30\n",
      " - 129s - loss: 4.0954 - acc: 0.1218 - val_loss: 3.9020 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 155s - loss: 4.4659 - acc: 0.1257 - val_loss: 4.0781 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 142s - loss: 4.0544 - acc: 0.1310 - val_loss: 4.0618 - val_acc: 0.1087\n",
      "Epoch 3/30\n",
      " - 142s - loss: 4.0421 - acc: 0.1341 - val_loss: 4.0583 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 142s - loss: 4.0367 - acc: 0.1340 - val_loss: 4.0650 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 142s - loss: 4.0359 - acc: 0.1330 - val_loss: 4.0696 - val_acc: 0.1049\n",
      "Epoch 6/30\n",
      " - 142s - loss: 4.0301 - acc: 0.1338 - val_loss: 4.0435 - val_acc: 0.1049\n",
      "Epoch 7/30\n",
      " - 142s - loss: 4.0269 - acc: 0.1346 - val_loss: 4.0563 - val_acc: 0.1049\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 174s - loss: 4.4181 - acc: 0.1218 - val_loss: 3.8822 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 161s - loss: 4.0611 - acc: 0.1258 - val_loss: 3.8786 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 161s - loss: 4.0544 - acc: 0.1261 - val_loss: 3.8918 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 161s - loss: 4.0492 - acc: 0.1271 - val_loss: 3.8803 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 161s - loss: 4.0470 - acc: 0.1260 - val_loss: 3.8638 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 161s - loss: 4.0436 - acc: 0.1264 - val_loss: 3.8804 - val_acc: 0.1406\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 176s - loss: 4.3759 - acc: 0.1253 - val_loss: 4.6821 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 162s - loss: 4.0211 - acc: 0.1274 - val_loss: 4.6868 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 162s - loss: 4.0120 - acc: 0.1288 - val_loss: 4.6962 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 162s - loss: 4.0076 - acc: 0.1294 - val_loss: 4.7066 - val_acc: 0.0806\n",
      "Epoch 5/30\n",
      " - 162s - loss: 4.0039 - acc: 0.1286 - val_loss: 4.7148 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 162s - loss: 4.0004 - acc: 0.1292 - val_loss: 4.7496 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Checking regstrength 0.01\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 173s - loss: 4.5892 - acc: 0.1221 - val_loss: 4.2665 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.0925 - acc: 0.1261 - val_loss: 4.2746 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 165s - loss: 4.0805 - acc: 0.1267 - val_loss: 4.2573 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 159s - loss: 4.0721 - acc: 0.1266 - val_loss: 4.2605 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 159s - loss: 4.0654 - acc: 0.1274 - val_loss: 4.2644 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 159s - loss: 4.0604 - acc: 0.1264 - val_loss: 4.2510 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 173s - loss: 4.6450 - acc: 0.1204 - val_loss: 3.8357 - val_acc: 0.1530\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.1325 - acc: 0.1252 - val_loss: 3.8600 - val_acc: 0.0954\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.1225 - acc: 0.1243 - val_loss: 3.8222 - val_acc: 0.1530\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.1153 - acc: 0.1236 - val_loss: 3.8407 - val_acc: 0.1530\n",
      "Epoch 5/30\n",
      " - 157s - loss: 4.1044 - acc: 0.1249 - val_loss: 3.8191 - val_acc: 0.0954\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.1002 - acc: 0.1256 - val_loss: 3.8452 - val_acc: 0.1530\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 173s - loss: 4.6289 - acc: 0.1234 - val_loss: 4.0325 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 157s - loss: 4.1114 - acc: 0.1285 - val_loss: 4.0415 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 157s - loss: 4.0996 - acc: 0.1249 - val_loss: 4.0180 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 157s - loss: 4.0889 - acc: 0.1272 - val_loss: 4.0573 - val_acc: 0.1316\n",
      "Epoch 5/30\n",
      " - 157s - loss: 4.0850 - acc: 0.1259 - val_loss: 4.0461 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 157s - loss: 4.0747 - acc: 0.1279 - val_loss: 4.0467 - val_acc: 0.1316\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 175s - loss: 4.5730 - acc: 0.1252 - val_loss: 4.5514 - val_acc: 0.1139\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.0609 - acc: 0.1283 - val_loss: 4.5625 - val_acc: 0.1139\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.0481 - acc: 0.1276 - val_loss: 4.5477 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.0353 - acc: 0.1264 - val_loss: 4.5715 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.0280 - acc: 0.1289 - val_loss: 4.5860 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.0187 - acc: 0.1267 - val_loss: 4.5904 - val_acc: 0.1139\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 179s - loss: 4.5942 - acc: 0.1219 - val_loss: 4.2765 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 162s - loss: 4.0950 - acc: 0.1254 - val_loss: 4.2584 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 162s - loss: 4.0849 - acc: 0.1253 - val_loss: 4.2781 - val_acc: 0.1564\n",
      "Epoch 4/30\n",
      " - 162s - loss: 4.0768 - acc: 0.1247 - val_loss: 4.2854 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 162s - loss: 4.0679 - acc: 0.1261 - val_loss: 4.2803 - val_acc: 0.1564\n",
      "Epoch 6/30\n",
      " - 162s - loss: 4.0602 - acc: 0.1259 - val_loss: 4.2809 - val_acc: 0.1564\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 148s - loss: 4.8402 - acc: 0.1141 - val_loss: 3.9817 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 129s - loss: 4.1722 - acc: 0.1205 - val_loss: 3.9813 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 129s - loss: 4.1602 - acc: 0.1209 - val_loss: 3.9563 - val_acc: 0.1432\n",
      "Epoch 4/30\n",
      " - 130s - loss: 4.1526 - acc: 0.1192 - val_loss: 3.9475 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 129s - loss: 4.1447 - acc: 0.1216 - val_loss: 3.9555 - val_acc: 0.1432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      " - 130s - loss: 4.1393 - acc: 0.1217 - val_loss: 3.9602 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 161s - loss: 4.6785 - acc: 0.1312 - val_loss: 4.1330 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 142s - loss: 4.1045 - acc: 0.1331 - val_loss: 4.1173 - val_acc: 0.1049\n",
      "Epoch 3/30\n",
      " - 142s - loss: 4.0955 - acc: 0.1343 - val_loss: 4.1117 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 142s - loss: 4.0844 - acc: 0.1339 - val_loss: 4.1087 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 142s - loss: 4.0765 - acc: 0.1343 - val_loss: 4.1149 - val_acc: 0.1049\n",
      "Epoch 6/30\n",
      " - 142s - loss: 4.0707 - acc: 0.1339 - val_loss: 4.1230 - val_acc: 0.1049\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 182s - loss: 4.6054 - acc: 0.1239 - val_loss: 3.9302 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 161s - loss: 4.1135 - acc: 0.1249 - val_loss: 3.9386 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 161s - loss: 4.1049 - acc: 0.1270 - val_loss: 3.9439 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 161s - loss: 4.0953 - acc: 0.1253 - val_loss: 3.9189 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 161s - loss: 4.0874 - acc: 0.1270 - val_loss: 3.9222 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 161s - loss: 4.0842 - acc: 0.1275 - val_loss: 3.9187 - val_acc: 0.1406\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 183s - loss: 4.5647 - acc: 0.1223 - val_loss: 4.7018 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 162s - loss: 4.0755 - acc: 0.1289 - val_loss: 4.7263 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 162s - loss: 4.0631 - acc: 0.1304 - val_loss: 4.7165 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 162s - loss: 4.0540 - acc: 0.1297 - val_loss: 4.7441 - val_acc: 0.0790\n",
      "Epoch 5/30\n",
      " - 162s - loss: 4.0460 - acc: 0.1282 - val_loss: 4.7481 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 163s - loss: 4.0411 - acc: 0.1281 - val_loss: 4.7504 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Checking regstrength 0.05\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 180s - loss: 6.1063 - acc: 0.1260 - val_loss: 4.5419 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.3315 - acc: 0.1277 - val_loss: 4.5096 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.2836 - acc: 0.1289 - val_loss: 4.4419 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.2400 - acc: 0.1268 - val_loss: 4.4323 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2052 - acc: 0.1292 - val_loss: 4.3978 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.1752 - acc: 0.1281 - val_loss: 4.3906 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 180s - loss: 6.1561 - acc: 0.1218 - val_loss: 4.1103 - val_acc: 0.1530\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.3755 - acc: 0.1251 - val_loss: 4.0653 - val_acc: 0.1530\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.3228 - acc: 0.1257 - val_loss: 4.0644 - val_acc: 0.1530\n",
      "Epoch 4/30\n",
      " - 157s - loss: 4.2829 - acc: 0.1244 - val_loss: 3.9858 - val_acc: 0.1530\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2446 - acc: 0.1257 - val_loss: 3.9760 - val_acc: 0.1530\n",
      "Epoch 6/30\n",
      " - 157s - loss: 4.2150 - acc: 0.1248 - val_loss: 3.9550 - val_acc: 0.1530\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 179s - loss: 6.1510 - acc: 0.1257 - val_loss: 4.2892 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 156s - loss: 4.3544 - acc: 0.1265 - val_loss: 4.2589 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 156s - loss: 4.3008 - acc: 0.1277 - val_loss: 4.2290 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 156s - loss: 4.2590 - acc: 0.1279 - val_loss: 4.1914 - val_acc: 0.1316\n",
      "Epoch 5/30\n",
      " - 156s - loss: 4.2229 - acc: 0.1276 - val_loss: 4.1851 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 156s - loss: 4.1925 - acc: 0.1265 - val_loss: 4.1637 - val_acc: 0.1316\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 180s - loss: 6.1115 - acc: 0.1257 - val_loss: 4.7780 - val_acc: 0.1139\n",
      "Epoch 2/30\n",
      " - 157s - loss: 4.3018 - acc: 0.1294 - val_loss: 4.7483 - val_acc: 0.1139\n",
      "Epoch 3/30\n",
      " - 156s - loss: 4.2478 - acc: 0.1283 - val_loss: 4.7071 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 156s - loss: 4.2037 - acc: 0.1293 - val_loss: 4.6899 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 156s - loss: 4.1671 - acc: 0.1283 - val_loss: 4.6826 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 156s - loss: 4.1360 - acc: 0.1298 - val_loss: 4.6762 - val_acc: 0.1139\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 185s - loss: 6.0580 - acc: 0.1241 - val_loss: 4.4901 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 160s - loss: 4.3366 - acc: 0.1268 - val_loss: 4.4406 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 160s - loss: 4.2856 - acc: 0.1251 - val_loss: 4.4478 - val_acc: 0.1564\n",
      "Epoch 4/30\n",
      " - 160s - loss: 4.2406 - acc: 0.1248 - val_loss: 4.4000 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 160s - loss: 4.2064 - acc: 0.1249 - val_loss: 4.3939 - val_acc: 0.1564\n",
      "Epoch 6/30\n",
      " - 160s - loss: 4.1729 - acc: 0.1244 - val_loss: 4.3721 - val_acc: 0.1564\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 158s - loss: 6.7355 - acc: 0.1216 - val_loss: 4.2572 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 130s - loss: 4.4271 - acc: 0.1220 - val_loss: 4.2143 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 130s - loss: 4.3832 - acc: 0.1224 - val_loss: 4.1858 - val_acc: 0.1432\n",
      "Epoch 4/30\n",
      " - 130s - loss: 4.3478 - acc: 0.1217 - val_loss: 4.1469 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 130s - loss: 4.3116 - acc: 0.1201 - val_loss: 4.1302 - val_acc: 0.1432\n",
      "Epoch 6/30\n",
      " - 130s - loss: 4.2848 - acc: 0.1223 - val_loss: 4.1247 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 169s - loss: 6.3590 - acc: 0.1329 - val_loss: 4.4284 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 143s - loss: 4.3505 - acc: 0.1339 - val_loss: 4.3560 - val_acc: 0.1049\n",
      "Epoch 3/30\n",
      " - 143s - loss: 4.3026 - acc: 0.1349 - val_loss: 4.3400 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 143s - loss: 4.2621 - acc: 0.1348 - val_loss: 4.3029 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 143s - loss: 4.2276 - acc: 0.1347 - val_loss: 4.2639 - val_acc: 0.1049\n",
      "Epoch 6/30\n",
      " - 143s - loss: 4.2009 - acc: 0.1335 - val_loss: 4.2331 - val_acc: 0.1049\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 190s - loss: 6.0931 - acc: 0.1246 - val_loss: 4.2139 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 161s - loss: 4.3551 - acc: 0.1275 - val_loss: 4.1689 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 161s - loss: 4.3055 - acc: 0.1274 - val_loss: 4.1054 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 164s - loss: 4.2618 - acc: 0.1270 - val_loss: 4.0791 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 161s - loss: 4.2257 - acc: 0.1270 - val_loss: 4.0663 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 161s - loss: 4.1964 - acc: 0.1259 - val_loss: 4.0355 - val_acc: 0.1406\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 192s - loss: 6.0315 - acc: 0.1283 - val_loss: 4.9199 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 163s - loss: 4.3167 - acc: 0.1297 - val_loss: 4.9119 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 163s - loss: 4.2605 - acc: 0.1290 - val_loss: 4.8446 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 163s - loss: 4.2187 - acc: 0.1298 - val_loss: 4.8480 - val_acc: 0.0806\n",
      "Epoch 5/30\n",
      " - 163s - loss: 4.1806 - acc: 0.1313 - val_loss: 4.8001 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 163s - loss: 4.1512 - acc: 0.1300 - val_loss: 4.8197 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Checking regstrength 0.1\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 188s - loss: 7.9556 - acc: 0.1272 - val_loss: 4.7388 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.4910 - acc: 0.1280 - val_loss: 4.6321 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.3950 - acc: 0.1272 - val_loss: 4.5334 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.3173 - acc: 0.1293 - val_loss: 4.4879 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2584 - acc: 0.1283 - val_loss: 4.4346 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.2077 - acc: 0.1283 - val_loss: 4.3718 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 188s - loss: 7.9425 - acc: 0.1245 - val_loss: 4.3183 - val_acc: 0.1530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      " - 158s - loss: 4.5305 - acc: 0.1258 - val_loss: 4.2249 - val_acc: 0.1530\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.4346 - acc: 0.1266 - val_loss: 4.1492 - val_acc: 0.0954\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.3584 - acc: 0.1253 - val_loss: 4.1046 - val_acc: 0.1530\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2985 - acc: 0.1265 - val_loss: 4.0362 - val_acc: 0.1530\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.2470 - acc: 0.1252 - val_loss: 3.9938 - val_acc: 0.1530\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 190s - loss: 7.9604 - acc: 0.1267 - val_loss: 4.4819 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.5101 - acc: 0.1268 - val_loss: 4.3822 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.4154 - acc: 0.1274 - val_loss: 4.3322 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.3367 - acc: 0.1276 - val_loss: 4.2714 - val_acc: 0.1316\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2748 - acc: 0.1268 - val_loss: 4.2357 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.2243 - acc: 0.1278 - val_loss: 4.1996 - val_acc: 0.1316\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 191s - loss: 7.9228 - acc: 0.1263 - val_loss: 4.9734 - val_acc: 0.0569\n",
      "Epoch 2/30\n",
      " - 159s - loss: 4.4599 - acc: 0.1291 - val_loss: 4.8621 - val_acc: 0.1139\n",
      "Epoch 3/30\n",
      " - 159s - loss: 4.3599 - acc: 0.1282 - val_loss: 4.7757 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.2821 - acc: 0.1289 - val_loss: 4.7533 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2164 - acc: 0.1287 - val_loss: 4.7037 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 159s - loss: 4.1675 - acc: 0.1282 - val_loss: 4.6673 - val_acc: 0.1139\n",
      "Epoch 7/30\n",
      " - 159s - loss: 4.1263 - acc: 0.1288 - val_loss: 4.6518 - val_acc: 0.1139\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 196s - loss: 7.8170 - acc: 0.1245 - val_loss: 4.6596 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 163s - loss: 4.4932 - acc: 0.1266 - val_loss: 4.5745 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 163s - loss: 4.3938 - acc: 0.1262 - val_loss: 4.5307 - val_acc: 0.1564\n",
      "Epoch 4/30\n",
      " - 163s - loss: 4.3133 - acc: 0.1259 - val_loss: 4.4650 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 163s - loss: 4.2523 - acc: 0.1262 - val_loss: 4.4131 - val_acc: 0.1564\n",
      "Epoch 6/30\n",
      " - 163s - loss: 4.2014 - acc: 0.1266 - val_loss: 4.3744 - val_acc: 0.1564\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 166s - loss: 9.0513 - acc: 0.1191 - val_loss: 4.4588 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 132s - loss: 4.6023 - acc: 0.1221 - val_loss: 4.3659 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 132s - loss: 4.5205 - acc: 0.1216 - val_loss: 4.3095 - val_acc: 0.1432\n",
      "Epoch 4/30\n",
      " - 132s - loss: 4.4511 - acc: 0.1220 - val_loss: 4.2558 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 132s - loss: 4.3933 - acc: 0.1232 - val_loss: 4.2052 - val_acc: 0.1432\n",
      "Epoch 6/30\n",
      " - 132s - loss: 4.3436 - acc: 0.1229 - val_loss: 4.1662 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 179s - loss: 8.3515 - acc: 0.1331 - val_loss: 4.6188 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 144s - loss: 4.5169 - acc: 0.1352 - val_loss: 4.5021 - val_acc: 0.1049\n",
      "Epoch 3/30\n",
      " - 144s - loss: 4.4278 - acc: 0.1334 - val_loss: 4.4457 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 144s - loss: 4.3535 - acc: 0.1352 - val_loss: 4.3980 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 144s - loss: 4.2920 - acc: 0.1352 - val_loss: 4.3271 - val_acc: 0.1049\n",
      "Epoch 6/30\n",
      " - 144s - loss: 4.2449 - acc: 0.1347 - val_loss: 4.2861 - val_acc: 0.1049\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 199s - loss: 7.8670 - acc: 0.1247 - val_loss: 4.4152 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 163s - loss: 4.5087 - acc: 0.1273 - val_loss: 4.3025 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 163s - loss: 4.4142 - acc: 0.1271 - val_loss: 4.2242 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 163s - loss: 4.3361 - acc: 0.1273 - val_loss: 4.1563 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 163s - loss: 4.2753 - acc: 0.1275 - val_loss: 4.1049 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 163s - loss: 4.2220 - acc: 0.1277 - val_loss: 4.0611 - val_acc: 0.1406\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 205s - loss: 7.7282 - acc: 0.1294 - val_loss: 5.0497 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 165s - loss: 4.4712 - acc: 0.1302 - val_loss: 4.9717 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 200s - loss: 4.3698 - acc: 0.1303 - val_loss: 4.9118 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 182s - loss: 4.2921 - acc: 0.1308 - val_loss: 4.8589 - val_acc: 0.0806\n",
      "Epoch 5/30\n",
      " - 165s - loss: 4.2289 - acc: 0.1317 - val_loss: 4.8429 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 165s - loss: 4.1776 - acc: 0.1311 - val_loss: 4.8206 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Checking regstrength 0.5\n",
      "\n",
      "Validating on opus 127\n",
      "Train on 25863 samples, validate on 2210 samples\n",
      "Epoch 1/30\n",
      " - 199s - loss: 21.4679 - acc: 0.1280 - val_loss: 5.2686 - val_acc: 0.1253\n",
      "Epoch 2/30\n",
      " - 160s - loss: 4.9162 - acc: 0.1282 - val_loss: 4.9329 - val_acc: 0.1253\n",
      "Epoch 3/30\n",
      " - 160s - loss: 4.6199 - acc: 0.1282 - val_loss: 4.6761 - val_acc: 0.1253\n",
      "Epoch 4/30\n",
      " - 160s - loss: 4.4154 - acc: 0.1272 - val_loss: 4.5099 - val_acc: 0.1253\n",
      "Epoch 5/30\n",
      " - 160s - loss: 4.2736 - acc: 0.1287 - val_loss: 4.4131 - val_acc: 0.1253\n",
      "Epoch 6/30\n",
      " - 160s - loss: 4.1747 - acc: 0.1271 - val_loss: 4.3380 - val_acc: 0.1253\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 130\n",
      "Train on 25609 samples, validate on 2464 samples\n",
      "Epoch 1/30\n",
      " - 198s - loss: 21.5198 - acc: 0.1247 - val_loss: 4.9524 - val_acc: 0.1530\n",
      "Epoch 2/30\n",
      " - 157s - loss: 4.9538 - acc: 0.1260 - val_loss: 4.5931 - val_acc: 0.1530\n",
      "Epoch 3/30\n",
      " - 157s - loss: 4.6620 - acc: 0.1259 - val_loss: 4.3306 - val_acc: 0.1530\n",
      "Epoch 4/30\n",
      " - 157s - loss: 4.4561 - acc: 0.1260 - val_loss: 4.1800 - val_acc: 0.1530\n",
      "Epoch 5/30\n",
      " - 157s - loss: 4.3151 - acc: 0.1254 - val_loss: 4.0510 - val_acc: 0.1530\n",
      "Epoch 6/30\n",
      " - 157s - loss: 4.2157 - acc: 0.1253 - val_loss: 3.9624 - val_acc: 0.1530\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 131\n",
      "Train on 25519 samples, validate on 2554 samples\n",
      "Epoch 1/30\n",
      " - 198s - loss: 21.4842 - acc: 0.1263 - val_loss: 5.0821 - val_acc: 0.1316\n",
      "Epoch 2/30\n",
      " - 157s - loss: 4.9412 - acc: 0.1286 - val_loss: 4.7140 - val_acc: 0.1316\n",
      "Epoch 3/30\n",
      " - 157s - loss: 4.6427 - acc: 0.1284 - val_loss: 4.4892 - val_acc: 0.1316\n",
      "Epoch 4/30\n",
      " - 157s - loss: 4.4365 - acc: 0.1279 - val_loss: 4.3437 - val_acc: 0.1316\n",
      "Epoch 5/30\n",
      " - 157s - loss: 4.2936 - acc: 0.1265 - val_loss: 4.2209 - val_acc: 0.1316\n",
      "Epoch 6/30\n",
      " - 157s - loss: 4.1951 - acc: 0.1287 - val_loss: 4.1401 - val_acc: 0.1316\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 132\n",
      "Train on 25649 samples, validate on 2424 samples\n",
      "Epoch 1/30\n",
      " - 199s - loss: 21.5039 - acc: 0.1292 - val_loss: 5.4344 - val_acc: 0.1139\n",
      "Epoch 2/30\n",
      " - 158s - loss: 4.8883 - acc: 0.1296 - val_loss: 5.1034 - val_acc: 0.1139\n",
      "Epoch 3/30\n",
      " - 158s - loss: 4.5874 - acc: 0.1300 - val_loss: 4.8841 - val_acc: 0.1139\n",
      "Epoch 4/30\n",
      " - 158s - loss: 4.3798 - acc: 0.1300 - val_loss: 4.7376 - val_acc: 0.1139\n",
      "Epoch 5/30\n",
      " - 158s - loss: 4.2362 - acc: 0.1293 - val_loss: 4.6570 - val_acc: 0.1139\n",
      "Epoch 6/30\n",
      " - 158s - loss: 4.1373 - acc: 0.1297 - val_loss: 4.5826 - val_acc: 0.1139\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 135\n",
      "Train on 26570 samples, validate on 1503 samples\n",
      "Epoch 1/30\n",
      " - 205s - loss: 21.0200 - acc: 0.1249 - val_loss: 5.1919 - val_acc: 0.1564\n",
      "Epoch 2/30\n",
      " - 162s - loss: 4.9031 - acc: 0.1274 - val_loss: 4.8742 - val_acc: 0.1564\n",
      "Epoch 3/30\n",
      " - 163s - loss: 4.6023 - acc: 0.1273 - val_loss: 4.6295 - val_acc: 0.0765\n",
      "Epoch 4/30\n",
      " - 162s - loss: 4.3984 - acc: 0.1265 - val_loss: 4.4787 - val_acc: 0.1564\n",
      "Epoch 5/30\n",
      " - 162s - loss: 4.2588 - acc: 0.1268 - val_loss: 4.3986 - val_acc: 0.0765\n",
      "Epoch 6/30\n",
      " - 162s - loss: 4.1641 - acc: 0.1258 - val_loss: 4.3178 - val_acc: 0.1564\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 18\n",
      "Train on 19584 samples, validate on 8489 samples\n",
      "Epoch 1/30\n",
      " - 177s - loss: 26.5913 - acc: 0.1214 - val_loss: 5.1299 - val_acc: 0.1432\n",
      "Epoch 2/30\n",
      " - 165s - loss: 5.1191 - acc: 0.1226 - val_loss: 4.8265 - val_acc: 0.1432\n",
      "Epoch 3/30\n",
      " - 144s - loss: 4.8519 - acc: 0.1226 - val_loss: 4.5899 - val_acc: 0.1432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      " - 137s - loss: 4.6497 - acc: 0.1228 - val_loss: 4.4328 - val_acc: 0.1432\n",
      "Epoch 5/30\n",
      " - 131s - loss: 4.4964 - acc: 0.1228 - val_loss: 4.3113 - val_acc: 0.1051\n",
      "Epoch 6/30\n",
      " - 132s - loss: 4.3798 - acc: 0.1229 - val_loss: 4.2002 - val_acc: 0.1432\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 59\n",
      "Train on 22514 samples, validate on 5559 samples\n",
      "Epoch 1/30\n",
      " - 189s - loss: 23.8109 - acc: 0.1342 - val_loss: 5.2206 - val_acc: 0.1049\n",
      "Epoch 2/30\n",
      " - 144s - loss: 4.9877 - acc: 0.1352 - val_loss: 4.8969 - val_acc: 0.1049\n",
      "Epoch 3/30\n",
      " - 144s - loss: 4.7059 - acc: 0.1349 - val_loss: 4.6324 - val_acc: 0.1049\n",
      "Epoch 4/30\n",
      " - 144s - loss: 4.5010 - acc: 0.1344 - val_loss: 4.4866 - val_acc: 0.1049\n",
      "Epoch 5/30\n",
      " - 144s - loss: 4.3532 - acc: 0.1352 - val_loss: 4.3577 - val_acc: 0.1049\n",
      "Epoch 6/30\n",
      " - 144s - loss: 4.2450 - acc: 0.1351 - val_loss: 4.2386 - val_acc: 0.1049\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 74\n",
      "Train on 26558 samples, validate on 1515 samples\n",
      "Epoch 1/30\n",
      " - 207s - loss: 20.9321 - acc: 0.1267 - val_loss: 4.9839 - val_acc: 0.1406\n",
      "Epoch 2/30\n",
      " - 173s - loss: 4.9225 - acc: 0.1278 - val_loss: 4.6182 - val_acc: 0.1406\n",
      "Epoch 3/30\n",
      " - 177s - loss: 4.6274 - acc: 0.1281 - val_loss: 4.3976 - val_acc: 0.1406\n",
      "Epoch 4/30\n",
      " - 172s - loss: 4.4224 - acc: 0.1281 - val_loss: 4.2094 - val_acc: 0.1406\n",
      "Epoch 5/30\n",
      " - 161s - loss: 4.2828 - acc: 0.1271 - val_loss: 4.0911 - val_acc: 0.1406\n",
      "Epoch 6/30\n",
      " - 161s - loss: 4.1870 - acc: 0.1275 - val_loss: 4.0483 - val_acc: 0.0950\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Validating on opus 95\n",
      "Train on 26795 samples, validate on 1278 samples\n",
      "Epoch 1/30\n",
      " - 210s - loss: 20.7011 - acc: 0.1303 - val_loss: 5.4883 - val_acc: 0.0806\n",
      "Epoch 2/30\n",
      " - 162s - loss: 4.8800 - acc: 0.1313 - val_loss: 5.1785 - val_acc: 0.0806\n",
      "Epoch 3/30\n",
      " - 162s - loss: 4.5778 - acc: 0.1300 - val_loss: 4.9945 - val_acc: 0.0806\n",
      "Epoch 4/30\n",
      " - 162s - loss: 4.3711 - acc: 0.1314 - val_loss: 4.8663 - val_acc: 0.0806\n",
      "Epoch 5/30\n",
      " - 162s - loss: 4.2328 - acc: 0.1313 - val_loss: 4.7760 - val_acc: 0.0806\n",
      "Epoch 6/30\n",
      " - 162s - loss: 4.1372 - acc: 0.1316 - val_loss: 4.7212 - val_acc: 0.0806\n",
      "Epoch 00006: early stopping\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#define the range of regularization strengths to check\n",
    "regstrength = [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "print(\"Start!\")\n",
    "\n",
    "#Create container for results\n",
    "RESULTS = pd.DataFrame()\n",
    "\n",
    "for strength in regstrength:\n",
    "    print(\"\\nChecking regstrength {}\".format(strength))\n",
    "    cv_results = pd.DataFrame()\n",
    "    \n",
    "    #Cross validate on each opus\n",
    "    for opus in data['op'].unique():\n",
    "        print(\"\\nValidating on opus {}\".format(opus))\n",
    "\n",
    "        #Split into training and validation\n",
    "        valid = data[data['op'] == opus]\n",
    "        train = data[data['op'] != opus]\n",
    "\n",
    "        #Drop the opus attribute since it's no longer needed\n",
    "        valid = valid.drop(columns='op')\n",
    "        train = train.drop(columns='op')\n",
    "\n",
    "        #Generate sequences from the data\n",
    "        valid_in, valid_out = generate_sequences(valid, valid, seq_length)\n",
    "        train_in, train_out = generate_sequences(train, train, seq_length)\n",
    "\n",
    "        #Create model\n",
    "        model = lstm(train_in, train_out, optimizer, loss, metrics, strength)\n",
    "\n",
    "        #Train on the folds\n",
    "        model.fit(train_in,\n",
    "                  train_out,\n",
    "                  epochs = epochs,\n",
    "                  verbose = verbose,\n",
    "                  validation_data = (valid_in, valid_out),\n",
    "                  callbacks = callbacks_list)\n",
    "\n",
    "        #Save the history object for the model, appending test opus and regstrength\n",
    "        history = pd.DataFrame(model.history.history)\n",
    "        history.index.name = 'epoch'\n",
    "        history['opus'] = opus\n",
    "        history['reg'] = strength\n",
    "        cv_results = cv_results.append(history)\n",
    "    \n",
    "    RESULTS = RESULTS.append(cv_results)\n",
    "\n",
    "print(\"Done!\")\n",
    "BACKUP = RESULTS\n",
    "pd.DataFrame.to_csv(BACKUP, './results/BACKUP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore result from backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = pd.read_csv('results/BACKUP.csv')\n",
    "RESULTS = RESULTS.set_index(['reg','opus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each regularization value, calculate the average weighed cross-validated score and output it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full table of cross validated scores for each regularization value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000</th>\n",
       "      <td>3.641026</td>\n",
       "      <td>4.167775</td>\n",
       "      <td>0.131346</td>\n",
       "      <td>3.936454</td>\n",
       "      <td>0.133513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>2.528302</td>\n",
       "      <td>4.103910</td>\n",
       "      <td>0.127975</td>\n",
       "      <td>4.055110</td>\n",
       "      <td>0.124874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.005</th>\n",
       "      <td>2.404255</td>\n",
       "      <td>4.172407</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>4.106369</td>\n",
       "      <td>0.124472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>2.490196</td>\n",
       "      <td>4.194825</td>\n",
       "      <td>0.127646</td>\n",
       "      <td>4.183819</td>\n",
       "      <td>0.126107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.366877</td>\n",
       "      <td>0.127714</td>\n",
       "      <td>4.586606</td>\n",
       "      <td>0.127213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>2.622642</td>\n",
       "      <td>4.457916</td>\n",
       "      <td>0.127237</td>\n",
       "      <td>4.910315</td>\n",
       "      <td>0.127721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500</th>\n",
       "      <td>2.400000</td>\n",
       "      <td>4.655938</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>7.689694</td>\n",
       "      <td>0.128297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          epoch  val_loss   val_acc      loss       acc\n",
       "0.000  3.641026  4.167775  0.131346  3.936454  0.133513\n",
       "0.001  2.528302  4.103910  0.127975  4.055110  0.124874\n",
       "0.005  2.404255  4.172407  0.129148  4.106369  0.124472\n",
       "0.010  2.490196  4.194825  0.127646  4.183819  0.126107\n",
       "0.050  2.500000  4.366877  0.127714  4.586606  0.127213\n",
       "0.100  2.622642  4.457916  0.127237  4.910315  0.127721\n",
       "0.500  2.400000  4.655938  0.126000  7.689694  0.128297"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>3.641026</td>\n",
       "      <td>4.167775</td>\n",
       "      <td>0.131346</td>\n",
       "      <td>3.936454</td>\n",
       "      <td>0.133513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        epoch  val_loss   val_acc      loss       acc\n",
       "0.0  3.641026  4.167775  0.131346  3.936454  0.133513"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AVERAGES = pd.DataFrame()\n",
    "\n",
    "#For each level of regularization\n",
    "for regularization, cvscores in RESULTS.groupby(level=0):\n",
    "    average = pd.DataFrame()\n",
    "    \n",
    "    #Iterate through all folds and extract the highest weighed validation scores\n",
    "    for opus, fold in cvscores.groupby(level=1):\n",
    "        \n",
    "        #Weigh all metrics by the amount of chords in the opus validated on\n",
    "        #weight = (data[data['op'] == opus]).shape[0]\n",
    "        #fold = fold[['val_loss', 'val_acc', 'loss', 'acc']] * weight\n",
    "        \n",
    "        #Retrieve the best score\n",
    "        best = fold[fold['val_acc'] == fold['val_acc'].max()]\n",
    "        average = average.append(best)\n",
    "    \n",
    "    #Make a pretty dataframe of the mean\n",
    "    average = average.describe().loc[['mean']]\n",
    "    average = average.rename(index={'mean': regularization})\n",
    "    \n",
    "    #Take the mean scores for this regularization value and store them in AVERAGE for comparisons\n",
    "    AVERAGES = AVERAGES.append(average)\n",
    "\n",
    "BEST = AVERAGES[AVERAGES['val_acc'] == AVERAGES['val_acc'].max()]\n",
    "\n",
    "print(\"Full table of cross validated scores for each regularization value\")\n",
    "display(AVERAGES)\n",
    "\n",
    "print(\"Best score\")\n",
    "display(BEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighed validation accuracy for each opus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the data from the reg_value that produced the best results\n",
    "best_reg = BEST.index.values[0]\n",
    "df = RESULTS.loc[best_reg]\n",
    "\n",
    "#For each of the nine folds/opuses...\n",
    "scores = pd.DataFrame()\n",
    "for opus, fold in df.groupby(level=0):\n",
    "        \n",
    "        #Drop potential duplicate values\n",
    "        fold = fold.drop_duplicates(subset='val_acc')\n",
    "        \n",
    "        #Retrieve the scores when the val_acc was highest\n",
    "        valacc = fold[fold['val_acc'] == fold['val_acc'].max()]\n",
    "        \n",
    "        #Store\n",
    "        scores = scores.append(valacc)\n",
    "\n",
    "#Sort scores by valacc\n",
    "scores = scores.sort_values(by='val_acc', ascending=False)\n",
    "\n",
    "#Append the average score to the individual scores\n",
    "avg = BEST.rename(index={best_reg: 'Average'})\n",
    "scores = scores.append(avg)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#Plot them\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.barplot(data=scores[['val_acc']].T)\n",
    "ax.set_title(\"Accuracy on each opus\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.set_xlabel(\"Opus\")\n",
    "ax.set_yticklabels(np.around(ax.get_yticks()* 100,2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,15))\n",
    "sns.barplot(data=scores[['val_acc']].T, orient='horizontal')\n",
    "ax.set_title(\"Accuracy on each opus\")\n",
    "ax.set_xlabel(\"Accuracy (%)\")\n",
    "ax.set_ylabel(\"Opus\")\n",
    "ax.set_xticklabels(np.around(ax.get_xticks()* 100,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weigh all metrics by the amount of chords in the opus validated on\n",
    "#weight = (data[data['op'] == opus]).shape[0]\n",
    "#fold = fold[['val_loss', 'val_acc', 'loss', 'acc']] * weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
